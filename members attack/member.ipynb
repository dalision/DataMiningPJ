{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 乳腺癌检测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,RandomForestClassifier #GradientBoosting\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import BernoulliRBM  #神经网络\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 0 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 数据格式转换（一开始的数据是txt格式的）\n",
    "columns_name = [\"Sample code number\", \"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\", \"Class\"]\n",
    "\n",
    "dataset = pd.read_table('breast-cancer-wisconsin.txt',sep=',',names=columns_name,header=None)\n",
    "\n",
    "temp = dataset.head(500)\n",
    "train_dataset = pd.concat([temp.sample(400), dataset.iloc[500:600, :]])\n",
    "shadow_dataset = pd.concat([temp.sample(400), dataset.iloc[600:699, :]])\n",
    "\n",
    "train_dataset.to_csv(\"data/dataset.csv\", index=None)\n",
    "shadow_dataset.to_csv(\"data/dataset_shadow.csv\", index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 查看两个数据集的重复率\n",
    "train_dataset = pd.read_csv(\"work/dataset.csv\")\n",
    "shadow_dataset = pd.read_csv(\"work/dataset_shadow.csv\")\n",
    "# print(len())\n",
    "\n",
    "list1 = train_dataset[\"Sample code number\"]\n",
    "list2 = shadow_dataset[\"Sample code number\"]\n",
    "\n",
    "repeat_list = [e for e in list1 if e in list2.values]# 查看训练数据集中有多少元素在影子模型的训练集中\n",
    "len(repeat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练数据集class可视化\n",
    "x = [0,1]\n",
    "y = [len([e for e in train_dataset[\"Class\"].values if e==2]),len([e for e in train_dataset[\"Class\"].values if e==4])]\n",
    "_ = plt.bar(x,y,tick_label=['normal', 'maligant'], color=[\"steelblue\", \"#d9534f\"])\n",
    "for a,b in zip(x,y):\n",
    "    _ = plt.text(a,b,b,ha = 'center',va = 'bottom',fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 目标模型部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1227081</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1135090</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>695091</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>867392</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>807657</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>428903</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1173514</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1223426</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>560680</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1258549</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1100524</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1182404</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>1186936</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1299596</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1212422</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1080233</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>1321348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1295508</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1183246</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1171578</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1115293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>846423</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>191250</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1299924</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>1311875</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>1313982</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1106095</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1296025</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>342245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1238186</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>486283</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>743348</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1061990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1228311</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1239232</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1180523</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1190485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1158247</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>183913</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1047630</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1228152</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>128059</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>369565</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1151734</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>555977</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1199219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>684955</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1184241</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>704097</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>616240</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>378275</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1238021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1115282</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>636437</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1217717</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1255384</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1212232</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>837082</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1116192</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>837480</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "237             1227081                3                        1   \n",
       "438             1135090                4                        1   \n",
       "477              695091                1                        1   \n",
       "431              867392                4                        2   \n",
       "428              807657                6                        1   \n",
       "20               428903                7                        2   \n",
       "353             1173514                1                        1   \n",
       "111             1223426                1                        1   \n",
       "232              560680                3                        1   \n",
       "292             1258549                9                       10   \n",
       "248             1100524                6                       10   \n",
       "47              1182404                4                        1   \n",
       "444             1186936                2                        1   \n",
       "251             1299596                6                        6   \n",
       "387             1212422                3                        1   \n",
       "214             1080233                7                        6   \n",
       "458             1321348                2                        1   \n",
       "407             1295508                1                        1   \n",
       "125             1183246                1                        1   \n",
       "441             1171578                3                        1   \n",
       "376             1115293                1                        1   \n",
       "249              846423               10                        6   \n",
       "140              191250               10                        4   \n",
       "411             1299924                5                        1   \n",
       "493             1311875                5                        1   \n",
       "455             1313982                4                        3   \n",
       "272             1106095                4                        1   \n",
       "331             1296025                4                        1   \n",
       "131              342245                1                        1   \n",
       "401             1238186                4                        1   \n",
       "..                  ...              ...                      ...   \n",
       "363              486283                3                        1   \n",
       "50               743348                3                        2   \n",
       "435             1061990                1                        1   \n",
       "378             1228311                1                        1   \n",
       "334             1239232                3                        3   \n",
       "70              1180523                3                        1   \n",
       "250             1190485                1                        1   \n",
       "285             1158247                1                        1   \n",
       "185              183913                1                        2   \n",
       "354             1047630                7                        4   \n",
       "106             1228152                8                        9   \n",
       "190              128059                1                        1   \n",
       "421              369565                4                        1   \n",
       "269             1151734               10                        8   \n",
       "391              555977                5                        6   \n",
       "193             1199219                1                        1   \n",
       "92               684955                2                        1   \n",
       "64              1184241                2                        1   \n",
       "241              704097                1                        1   \n",
       "152              616240                5                        3   \n",
       "474              378275               10                        9   \n",
       "194             1238021                1                        1   \n",
       "196             1115282                5                        3   \n",
       "305              636437                1                        1   \n",
       "484             1217717                5                        1   \n",
       "105             1255384                3                        2   \n",
       "371             1212232                5                        1   \n",
       "93               837082                2                        1   \n",
       "437             1116192                5                        1   \n",
       "430              837480                7                        4   \n",
       "\n",
       "     Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "237                         1                  3                            2   \n",
       "438                         1                  1                            2   \n",
       "477                         1                  1                            2   \n",
       "431                         2                  1                            2   \n",
       "428                         3                  2                            2   \n",
       "20                          4                  1                            3   \n",
       "353                         1                  1                            4   \n",
       "111                         1                  1                            2   \n",
       "232                         2                  1                            2   \n",
       "292                        10                 10                           10   \n",
       "248                        10                  2                            8   \n",
       "47                          1                  1                            2   \n",
       "444                         3                  2                            2   \n",
       "251                         6                  5                            4   \n",
       "387                         1                  1                            2   \n",
       "214                         6                  3                            2   \n",
       "458                         1                  1                            2   \n",
       "407                         1                  1                            2   \n",
       "125                         1                  1                            1   \n",
       "441                         1                  1                            2   \n",
       "376                         1                  1                            2   \n",
       "249                         3                  6                            4   \n",
       "140                         4                 10                            2   \n",
       "411                         1                  1                            2   \n",
       "493                         2                  1                            2   \n",
       "455                         1                  1                            2   \n",
       "272                         1                  3                            2   \n",
       "331                         2                  1                            2   \n",
       "131                         3                  1                            2   \n",
       "401                         1                  1                            2   \n",
       "..                        ...                ...                          ...   \n",
       "363                         1                  1                            2   \n",
       "50                          2                  1                            2   \n",
       "435                         3                  2                            2   \n",
       "378                         1                  1                            1   \n",
       "334                         2                  6                            3   \n",
       "70                          1                  1                            2   \n",
       "250                         1                  1                            2   \n",
       "285                         1                  1                            2   \n",
       "185                         2                  1                            2   \n",
       "354                         6                  4                            6   \n",
       "106                         9                  5                            3   \n",
       "190                         1                  1                            2   \n",
       "421                         1                  1                            3   \n",
       "269                         7                  4                            3   \n",
       "391                         6                  8                            6   \n",
       "193                         1                  2                            1   \n",
       "92                          1                  1                            3   \n",
       "64                          1                  1                            2   \n",
       "241                         1                  1                            1   \n",
       "152                         4                  3                            4   \n",
       "474                         7                  3                            4   \n",
       "194                         1                  1                            2   \n",
       "196                         5                  5                            3   \n",
       "305                         1                  1                            2   \n",
       "484                         1                  6                            3   \n",
       "105                         2                  3                            2   \n",
       "371                         1                  1                            2   \n",
       "93                          1                  1                            2   \n",
       "437                         2                  1                            2   \n",
       "430                         4                  3                            4   \n",
       "\n",
       "    Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "237           1                1                1        1      2  \n",
       "438           1                2                1        1      2  \n",
       "477           1                2                1        1      2  \n",
       "431           1                2                1        1      2  \n",
       "428           1                1                1        1      2  \n",
       "20            4                3                3        1      4  \n",
       "353           3                1                1        1      2  \n",
       "111           1                3                1        1      2  \n",
       "232           1                2                1        1      2  \n",
       "292          10               10               10        1      4  \n",
       "248          10                7                3        3      4  \n",
       "47            1                2                1        1      2  \n",
       "444           1                2                1        1      2  \n",
       "251          10                7                6        2      4  \n",
       "387           1                3                1        1      2  \n",
       "214          10                7                1        1      4  \n",
       "458           1                2                1        1      2  \n",
       "407           4                1                1        1      2  \n",
       "125         NaN                2                1        1      2  \n",
       "441           1                1                1        1      2  \n",
       "376           1                1                1        1      2  \n",
       "249          10                7                8        4      4  \n",
       "140          10                5                3        3      4  \n",
       "411           1                2                1        1      2  \n",
       "493           1                1                1        1      2  \n",
       "455           1                4                8        1      2  \n",
       "272           1                3                1        1      2  \n",
       "331           1                1                1        1      2  \n",
       "131           1                1                1        1      2  \n",
       "401           1                2                1        1      2  \n",
       "..          ...              ...              ...      ...    ...  \n",
       "363           1                3                1        1      2  \n",
       "50            1                2                3        1      2  \n",
       "435           1                3                1        1      2  \n",
       "378           1                3                1        1      2  \n",
       "334           3                3                5        1      2  \n",
       "70            1                2                2        1      2  \n",
       "250           1                1                1        1      2  \n",
       "285           1                2                1        1      2  \n",
       "185           1                1                1        1      2  \n",
       "354           1                4                3        1      4  \n",
       "106           5                7                7        1      4  \n",
       "190           5                5                1        1      2  \n",
       "421           1                1                1        1      2  \n",
       "269          10                7                9        1      4  \n",
       "391          10                4               10        4      4  \n",
       "193           1                1                1        1      2  \n",
       "92            1                2                1        1      2  \n",
       "64            1                2                1        1      2  \n",
       "241           1                2                1        1      2  \n",
       "152           5                4                7        1      2  \n",
       "474           2                7                7        1      4  \n",
       "194           1                2                1        1      2  \n",
       "196           3                4               10        1      4  \n",
       "305           1                1                1        1      2  \n",
       "484           1                1                1        1      2  \n",
       "105           3                3                1        1      2  \n",
       "371           1                2                1        1      2  \n",
       "93            1                3                1        1      2  \n",
       "437           1                3                1        1      2  \n",
       "430          10                6                9        1      4  \n",
       "\n",
       "[500 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "train_dataset = pd.read_csv(\"work/dataset.csv\")\n",
    "# train_dataset = pd.read_csv(\"data/dataset_shadow.csv\")\n",
    "train_dataset = train_dataset.sample(frac=1)\n",
    "\n",
    "#替换所有的？为nan\n",
    "train_dataset = train_dataset.replace('?',np.nan) \n",
    "\n",
    "#数据标准化处理\n",
    "features_ss = train_dataset.loc[:,[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"]]\n",
    "\n",
    "ss = StandardScaler(copy=True, with_mean=True, with_std=True).fit(features_ss)\n",
    "train_dataset\n",
    "train_dataset.loc[:, [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"]] = ss.transform(features_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可视化并存储预测值和标签\n",
    "def plot_fig(xgbc, X_test, Y_test,X_train, Y_train):\n",
    "    y_pred = xgbc.predict(X_test)\n",
    "    xgbc_cm = confusion_matrix(Y_test, y_pred)\n",
    "    xgbc_cr = classification_report(Y_test, y_pred)\n",
    "    print(xgbc_cr)\n",
    "    print(\"Accuracy : %.6g\" % metrics.accuracy_score(Y_test.values, y_pred))\n",
    "    plt.show()\n",
    "    # 获取预测值,分类标签,成员标签,并保存在文件里\n",
    "    y_train_pred = xgbc.predict_proba(X_train)#预测值\n",
    "    t1=np.ones([len(y_train_pred),1])\n",
    "    t2=np.zeros([len(y_train_pred),1])\n",
    "    train_label=np.c_[t1,t2]#成员标签\n",
    "    y_train_pred=np.c_[y_train_pred,Y_train]\n",
    "\n",
    "    y_test_pred = xgbc.predict_proba(X_test)\n",
    "    t3=np.zeros([len(y_test_pred),1])\n",
    "    t4=np.ones([len(y_test_pred),1])\n",
    "    test_label=np.c_[t3,t4]\n",
    "    y_test_pred=np.c_[y_test_pred,Y_test]\n",
    "    \n",
    "    attack_X=[]\n",
    "    attack_Y=[]\n",
    "    Mattack_Y=[]\n",
    "    attack_X.append(y_train_pred)\n",
    "    attack_X.append(y_test_pred)\n",
    "    attack_X=np.vstack(attack_X)\n",
    "    attack_Y.append(train_label)\n",
    "    attack_Y.append(test_label)\n",
    "    attack_Y=np.vstack(attack_Y)\n",
    "    Mattack_Y.append(t1)\n",
    "    Mattack_Y.append(t3)\n",
    "    Mattack_Y=np.vstack(Mattack_Y)\n",
    "\n",
    "    # x_proba = clf.predict_proba(test_df)\n",
    "    np.savez(\"work/testset.npz\",attack_X,attack_Y)\n",
    "    np.savez(\"work/Mtestset.npz\",attack_X,Mattack_Y)\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_dataset):\n",
    "    # 分成5次训练\n",
    "    kf = KFold(n_splits=5)\n",
    "    print(\"------------------XGBClassifier is fitting......\")\n",
    "    for data_train, data_test in kf.split(train_dataset):\n",
    "        X_train = train_dataset.iloc[data_train, 1:-1]\n",
    "        Y_train = train_dataset.iloc[data_train, -1]\n",
    "        # print(len(X_train))\n",
    "        X_test = train_dataset.iloc[data_test, 1:-1]\n",
    "        Y_test = train_dataset.iloc[data_test, -1]\n",
    "        # print(len(X_test))\n",
    "        model.fit(X_train, Y_train)\n",
    "        plot_fig(model, X_test, Y_test,X_train,Y_train)\n",
    "\n",
    "    plot_importance(xgbc)\n",
    "    # 模型保存\n",
    "    joblib.dump(xgbc, './work/xgbc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------XGBClassifier is fitting......\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      0.98      0.98        65\n",
      "           4       0.97      0.94      0.96        35\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       100\n",
      "   macro avg       0.97      0.96      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n",
      "Accuracy : 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      0.97      0.99        71\n",
      "           4       0.94      1.00      0.97        29\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       100\n",
      "   macro avg       0.97      0.99      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "Accuracy : 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      0.98      0.99        55\n",
      "           4       0.98      1.00      0.99        45\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       100\n",
      "   macro avg       0.99      0.99      0.99       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n",
      "Accuracy : 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.93      0.97      0.95        58\n",
      "           4       0.95      0.90      0.93        42\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       100\n",
      "   macro avg       0.94      0.94      0.94       100\n",
      "weighted avg       0.94      0.94      0.94       100\n",
      "\n",
      "Accuracy : 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      0.96      0.97        73\n",
      "           4       0.90      0.96      0.93        27\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       100\n",
      "   macro avg       0.94      0.96      0.95       100\n",
      "weighted avg       0.96      0.96      0.96       100\n",
      "\n",
      "Accuracy : 0.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgbc = XGBClassifier(objective='binary:logistic', learning_rate=0.2, max_depth=4, n_estimators=40)\n",
    "train(xgbc,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "       min_child_w...pos_weight=None, subsample=None,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': range(40, 201, 20), 'max_depth': range(4, 11, 2), 'learning_rate': array([0.01   , 0.03111, 0.05222, 0.07333, 0.09444, 0.11556, 0.13667,\n",
       "       0.15778, 0.17889, 0.2    ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1788888888888889, 'max_depth': 4, 'n_estimators': 60}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型调参\n",
    "xgbc = XGBClassifier(objective='binary:logistic')\n",
    "param_dist = {\n",
    "        'n_estimators':range(40,201,20),\n",
    "        'max_depth':range(4,11,2),\n",
    "        'learning_rate':np.linspace(0.01,0.2,10),\n",
    "        }\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgbc, param_dist, cv = 5, iid=True, scoring='roc_auc', n_jobs=-1)\n",
    "clf.fit(train_dataset.iloc[:,1:-1], train_dataset.iloc[:, -1]) # gridSearch自带cross_validate,所以用全部数据训练即可\n",
    "np.save('./work/gridSearch.npy', clf.cv_results_)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 API查询接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "提示：请在一行中输入包括Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses在内的10个指标, 仅用英文逗号隔开.\n",
      "\n",
      "请输入: 您输入的指标对应为:\n",
      "+-----------------------------+--------+\n",
      "| Attribute                   | Domain |\n",
      "+-----------------------------+--------+\n",
      "| Clump Thickness             | 1      |\n",
      "+-----------------------------+--------+\n",
      "| Uniformity of Cell Size     | 2      |\n",
      "+-----------------------------+--------+\n",
      "| Uniformity of Cell Shape    | 3      |\n",
      "+-----------------------------+--------+\n",
      "| Marginal Adhesion           | 4      |\n",
      "+-----------------------------+--------+\n",
      "| Single Epithelial Cell Size | 5      |\n",
      "+-----------------------------+--------+\n",
      "| Bare Nuclei                 | 6      |\n",
      "+-----------------------------+--------+\n",
      "| Bland Chromatin             | 7      |\n",
      "+-----------------------------+--------+\n",
      "| Normal Nucleoli             | 8      |\n",
      "+-----------------------------+--------+\n",
      "| Mitoses                     | 9      |\n",
      "+-----------------------------+--------+\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8']\nexpected Uniformity of Cell Size, Normal Nucleoli, Marginal Adhesion, Clump Thickness, Uniformity of Cell Shape, Bland Chromatin, Bare Nuclei, Mitoses, Single Epithelial Cell Size in input data\ntraining data did not have the following fields: f3, f4, f6, f0, f1, f8, f2, f5, f7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4220b8121841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTexttable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Benign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Malignant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m    935\u001b[0m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[1;32m    936\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                                                  validate_features=validate_features)\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1854\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     def get_split_value_histogram(self, feature, fmap='', bins=None,\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8']\nexpected Uniformity of Cell Size, Normal Nucleoli, Marginal Adhesion, Clump Thickness, Uniformity of Cell Shape, Bland Chromatin, Bare Nuclei, Mitoses, Single Epithelial Cell Size in input data\ntraining data did not have the following fields: f3, f4, f6, f0, f1, f8, f2, f5, f7"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# \n",
    "\n",
    "from texttable import Texttable\n",
    "clf=joblib.load(\"./work/xgbc_model.pkl\")\n",
    "\n",
    "while(1):\n",
    "    print(\"=\"*100)\n",
    "    print(\"提示：请在一行中输入包括Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses在内的10个指标, 仅用英文逗号隔开.\\n\")\n",
    "    test_data = input(\"请输入: \")\n",
    "\n",
    "    if test_data == \"quit\":\n",
    "        break\n",
    "    \n",
    "    test_data = test_data.split(',')\n",
    "    test_data = [int(e) for e in test_data]\n",
    "\n",
    "    print(\"您输入的指标对应为:\")\n",
    "    t = Texttable()\n",
    "    col_names = [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\"]\n",
    "    _ = t.add_row([\"Attribute\", \"Domain\"])\n",
    "    for index in range(len(col_names)):\n",
    "        _ = t.add_row([col_names[index], test_data[index]])\n",
    "    print(t.draw())\n",
    "\n",
    "    test_df= pd.DataFrame([test_data], columns=[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"])\n",
    "    test_df = ss.transform(test_df)\n",
    "\n",
    "    x_proba = clf.predict_proba(test_df)\n",
    "    t2 = Texttable()\n",
    "    _ = t2.add_rows([[\"Benign\", \"Malignant\"], [x_proba[0][0], x_proba[0][1]]])\n",
    "    print(\"\\n您的检测结果为: \")\n",
    "    print(t2.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 阴影模型部分\n",
    "**1.数据预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\r\n",
    "train_dataset1 = pd.read_csv(\"work/dataset_shadow.csv\")\r\n",
    "train_dataset1= train_dataset1.sample(frac=1)\r\n",
    "\r\n",
    "#替换所有的？为'0'\r\n",
    "train_dataset1 = train_dataset1.replace('?','0') \r\n",
    "\r\n",
    "#数据标准化处理\r\n",
    "features_ss1 = train_dataset1.loc[:,[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"]]\r\n",
    "\r\n",
    "ss1 = StandardScaler(copy=True, with_mean=True, with_std=True).fit(features_ss1)\r\n",
    "# train_dataset\r\n",
    "train_dataset1.loc[:, [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"]] = ss1.transform(features_ss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 搭建阴影模型,因为不知道哪个最接近,所以都搭建\r\n",
    "def classifier1():\r\n",
    "    clf = SVC(C=0.5,                         #误差项惩罚系数,默认值是1\r\n",
    "                  kernel='linear',               #线性核 kenrel=\"rbf\":高斯核\r\n",
    "                  decision_function_shape='ovo')\r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier2():\r\n",
    "    clf = RandomForestClassifier(n_estimators=100)\r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier3():\r\n",
    "    #迭代100次 ,学习率为0.1\r\n",
    "    clf = AdaBoostClassifier(n_estimators=100,learning_rate=0.2)\r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier4():\r\n",
    "    #迭代100次 ,学习率为0.1\r\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier5():\r\n",
    "    logistic = linear_model.LogisticRegression()  \r\n",
    "    rbm = BernoulliRBM(random_state=0, verbose=True)  \r\n",
    "    clf = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])  \r\n",
    "    rbm.learning_rate = 0.1\r\n",
    "    rbm.n_iter = 20   \r\n",
    "    rbm.n_components = 100  \r\n",
    "    #正则化强度参数\r\n",
    "    logistic.C = 10   \r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier6():\r\n",
    "    clf = XGBClassifier(max_depth=3, n_estimators=100, learn_rate=0.1)\r\n",
    "    return clf\r\n",
    "\r\n",
    "def classifier7():\r\n",
    "    clf = linear_model.LogisticRegression()    # logistic回归\r\n",
    "    return clf \r\n",
    "\r\n",
    "# 训练函数\r\n",
    "# def train(clf,x_train,y_train):\r\n",
    "#     clf.fit(x_train,y_train.ravel())#ravel()将多维数据降成一维\r\n",
    "\r\n",
    "# 打印准确率和召回率\r\n",
    "def print_accuracy(clf,x_train,y_train,x_test,y_test):\r\n",
    "    #分别打印训练集和测试集的准确率  score(x_train,y_train):表示输出x_train,y_train在模型上的准确率\r\n",
    "    print('trianing prediction:%.6f' %(clf.score(x_train, y_train)))\r\n",
    "    print('testing prediction:%.6f' %(clf.score(x_test, y_test)))\r\n",
    "    # print(\"train_data recall:%.6f\"%recall_score(clf.predict(x_train), y_train))\r\n",
    "    # print(\"test_data recall:%.6f\"%recall_score(clf.predict(x_test), y_test))\r\n",
    "    test_pre=clf.predict(x_test)\r\n",
    "    cnt=0\r\n",
    "    for i in range(len(test_pre)):\r\n",
    "        if test_pre[i]==y_test[i]:\r\n",
    "            cnt+=1\r\n",
    "    print(\"漏报率:\",1-cnt/len(test_pre))\r\n",
    "    print(\"误报率:\",(len(test_pre)-cnt)/len(y_test))\r\n",
    "    target_names=['normal','malicious']\r\n",
    "    print(classification_report(y_test, clf.predict(x_test), target_names=target_names))\r\n",
    "    print(\"\\n\")\r\n",
    "\r\n",
    "#定义好分类模型\r\n",
    "clf1=classifier1()\r\n",
    "clf2=classifier2()\r\n",
    "clf3=classifier3()\r\n",
    "clf4=classifier4()\r\n",
    "# clf5=classifier5()\r\n",
    "clf6=classifier6()\r\n",
    "clf7=classifier7()\r\n",
    "\r\n",
    "\r\n",
    "def shadowTrain(targetclf,shadowclf,train_dataset,num=5):\r\n",
    "    # num是阴影模型的个数，越多得到的训练数据越多\r\n",
    "    for i in range(num):\r\n",
    "        kf = KFold(n_splits=2)\r\n",
    "        for data_train, data_test in kf.split(train_dataset):\r\n",
    "                x_train = train_dataset.iloc[data_train, 1:-1]\r\n",
    "                y_train = targetclf.predict(x_train)\r\n",
    "               \r\n",
    "                x_test = train_dataset.iloc[data_test, 1:-1]\r\n",
    "                y_test = targetclf.predict(x_test)\r\n",
    "            \r\n",
    "                shadowclf.fit(x_train,y_train)\r\n",
    "                print_accuracy(shadowclf,x_train,y_train,x_test,y_test)\r\n",
    "\r\n",
    "def shadowfile(shadowclf, X_test, Y_test,X_train, Y_train):\r\n",
    "    # 获取预测值,分类标签,成员标签,并保存在文件里\r\n",
    "    y_train_pred = shadowclf.predict_proba(X_train)#预测值\r\n",
    "    train_label=np.ones(len(y_train_pred))#成员标签\r\n",
    "    y_train_pred=np.c_[y_train_pred,Y_train,train_label]\r\n",
    "\r\n",
    "    y_test_pred = shadowclf.predict_proba(X_test)\r\n",
    "    test_label=np.zeros(len(y_test_pred))\r\n",
    "    y_test_pred=np.c_[y_test_pred,Y_test,test_label]\r\n",
    "    # x_proba = clf.predict_proba(test_df)\r\n",
    "    # print(\"y_train_pred[0]\")\r\n",
    "    # print(y_train_pred[0])\r\n",
    "    # print(\"y_test_pred[0]\")\r\n",
    "    # print(y_test_pred[0])\r\n",
    "    np.savez(\"work/trainset.npz\",y_train_pred,y_test_pred)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "trianing prediction:0.983936\n",
      "testing prediction:0.972000\n",
      "漏报率: 0.028000000000000025\n",
      "误报率: 0.028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.99      0.96      0.98       170\n",
      "   malicious       0.93      0.99      0.96        80\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       250\n",
      "   macro avg       0.96      0.98      0.97       250\n",
      "weighted avg       0.97      0.97      0.97       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:0.976000\n",
      "testing prediction:0.971888\n",
      "漏报率: 0.028112449799196804\n",
      "误报率: 0.028112449799196786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.96      0.99      0.98       155\n",
      "   malicious       0.99      0.94      0.96        94\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       249\n",
      "   macro avg       0.98      0.96      0.97       249\n",
      "weighted avg       0.97      0.97      0.97       249\n",
      "\n",
      "\n",
      "\n",
      "RandomForest:\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.972000\n",
      "漏报率: 0.028000000000000025\n",
      "误报率: 0.028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.99      0.96      0.98       170\n",
      "   malicious       0.93      0.99      0.96        80\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       250\n",
      "   macro avg       0.96      0.98      0.97       250\n",
      "weighted avg       0.97      0.97      0.97       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.963855\n",
      "漏报率: 0.03614457831325302\n",
      "误报率: 0.03614457831325301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.95      0.99      0.97       155\n",
      "   malicious       0.99      0.91      0.95        94\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       249\n",
      "   macro avg       0.97      0.95      0.96       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost:\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.980000\n",
      "漏报率: 0.020000000000000018\n",
      "误报率: 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       1.00      0.97      0.99       170\n",
      "   malicious       0.94      1.00      0.97        80\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       250\n",
      "   macro avg       0.97      0.99      0.98       250\n",
      "weighted avg       0.98      0.98      0.98       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.955823\n",
      "漏报率: 0.04417670682730923\n",
      "误报率: 0.04417670682730924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.99      0.97       155\n",
      "   malicious       0.98      0.90      0.94        94\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       249\n",
      "   macro avg       0.96      0.95      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "\n",
      "\n",
      "GradientBoosting:\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.972000\n",
      "漏报率: 0.028000000000000025\n",
      "误报率: 0.028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.99      0.96      0.98       170\n",
      "   malicious       0.93      0.99      0.96        80\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       250\n",
      "   macro avg       0.96      0.98      0.97       250\n",
      "weighted avg       0.97      0.97      0.97       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:1.000000\n",
      "testing prediction:0.951807\n",
      "漏报率: 0.048192771084337394\n",
      "误报率: 0.04819277108433735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.99      0.96       155\n",
      "   malicious       0.98      0.89      0.93        94\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       249\n",
      "   macro avg       0.96      0.94      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "\n",
      "\n",
      "xgboost:\n",
      "trianing prediction:0.995984\n",
      "testing prediction:0.964000\n",
      "漏报率: 0.03600000000000003\n",
      "误报率: 0.036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.98      0.96      0.97       170\n",
      "   malicious       0.93      0.96      0.94        80\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       250\n",
      "   macro avg       0.95      0.96      0.96       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:0.996000\n",
      "testing prediction:0.951807\n",
      "漏报率: 0.048192771084337394\n",
      "误报率: 0.04819277108433735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.93      0.99      0.96       155\n",
      "   malicious       0.99      0.88      0.93        94\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       249\n",
      "   macro avg       0.96      0.94      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression:\n",
      "trianing prediction:0.987952\n",
      "testing prediction:0.976000\n",
      "漏报率: 0.02400000000000002\n",
      "误报率: 0.024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.99      0.97      0.98       170\n",
      "   malicious       0.94      0.99      0.96        80\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       250\n",
      "   macro avg       0.97      0.98      0.97       250\n",
      "weighted avg       0.98      0.98      0.98       250\n",
      "\n",
      "\n",
      "\n",
      "trianing prediction:0.980000\n",
      "testing prediction:0.959839\n",
      "漏报率: 0.04016064257028118\n",
      "误报率: 0.040160642570281124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.99      0.97       155\n",
      "   malicious       0.99      0.90      0.94        94\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       249\n",
      "   macro avg       0.97      0.95      0.96       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择最相似的阴影模型\r\n",
    "print(\"SVM:\") \r\n",
    "shadowTrain(clf,clf1,train_dataset1,num=1)\r\n",
    "print(\"RandomForest:\")\r\n",
    "shadowTrain(clf,clf2,train_dataset1,num=1)\r\n",
    "print(\"AdaBoost:\") \r\n",
    "shadowTrain(clf,clf3,train_dataset1,num=1)\r\n",
    "print(\"GradientBoosting:\")\r\n",
    "shadowTrain(clf,clf4,train_dataset1,num=1)\r\n",
    "print(\"xgboost:\")\r\n",
    "shadowTrain(clf,clf6,train_dataset1,num=1)\r\n",
    "print(\"LogisticRegression:\")\r\n",
    "shadowTrain(clf,clf7,train_dataset1,num=1)\r\n",
    "# 最好的效果是xgboost和GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始训练阴影模型，并保存数据\r\n",
    "def shadowTrain2(targetclf,shadowclf,train_dataset,num=10):\r\n",
    "    # num是阴影模型的个数，越多得到的训练数据越多\r\n",
    "    attack_X=[]\r\n",
    "    attack_Y=[]\r\n",
    "    Mattack_Y=[]\r\n",
    "    for i in range(num):\r\n",
    "        kf = KFold(n_splits=5)\r\n",
    "        for data_train, data_test in kf.split(train_dataset):\r\n",
    "                x_train = train_dataset.iloc[data_train, 1:-1]\r\n",
    "                y_train = targetclf.predict(x_train)\r\n",
    "                # print(len(x_train))\r\n",
    "                x_test = train_dataset.iloc[data_test, 1:-1]\r\n",
    "                y_test = targetclf.predict(x_test)\r\n",
    "                # print(len(x_test))\r\n",
    "                shadowclf.fit(x_train,y_train)\r\n",
    "                attack_X_tmp,attack_Y_tmp,Mattack_Y_tmp=shadowfile(shadowclf,x_test,y_test,x_train,y_train)\r\n",
    "                attack_X.append(attack_X_tmp)\r\n",
    "                attack_Y.append(attack_Y_tmp)\r\n",
    "                Mattack_Y.append(Mattack_Y_tmp)\r\n",
    "    attack_X=np.vstack(attack_X)\r\n",
    "    attack_Y=np.vstack(attack_Y)\r\n",
    "    Mattack_Y=np.vstack(Mattack_Y)\r\n",
    "    np.savez(\"work/trainset.npz\",attack_X,attack_Y)\r\n",
    "    np.savez(\"work/Mtrainset.npz\",attack_X,Mattack_Y)\r\n",
    "\r\n",
    "def shadowfile(shadowclf, X_test, Y_test,X_train, Y_train):\r\n",
    "    # 获取预测值,分类标签,成员标签,并保存在文件里\r\n",
    "    y_train_pred = shadowclf.predict_proba(X_train)#预测值\r\n",
    "    t1=np.ones([len(y_train_pred),1])\r\n",
    "    t2=np.zeros([len(y_train_pred),1])\r\n",
    "    train_label=np.c_[t1,t2]#成员标签\r\n",
    "    y_train_pred=np.c_[y_train_pred,Y_train]\r\n",
    "\r\n",
    "    y_test_pred = shadowclf.predict_proba(X_test)\r\n",
    "    t3=np.zeros([len(y_test_pred),1])\r\n",
    "    t4=np.ones([len(y_test_pred),1])\r\n",
    "    test_label=np.c_[t3,t4]#成员标签\r\n",
    "    y_test_pred=np.c_[y_test_pred,Y_test]\r\n",
    "    \r\n",
    "    attack_X=[]\r\n",
    "    attack_Y=[]\r\n",
    "    Mattack_Y=[]\r\n",
    "    attack_X.append(y_train_pred)\r\n",
    "    attack_X.append(y_test_pred)\r\n",
    "    attack_X=np.vstack(attack_X)\r\n",
    "\r\n",
    "    attack_Y.append(train_label)\r\n",
    "    attack_Y.append(test_label)\r\n",
    "    attack_Y=np.vstack(attack_Y)\r\n",
    "\r\n",
    "    Mattack_Y.append(t1)\r\n",
    "    Mattack_Y.append(t3)\r\n",
    "    Mattack_Y=np.vstack(Mattack_Y)\r\n",
    "    # x_proba = clf.predict_proba(test_df)\r\n",
    "    return attack_X,attack_Y,Mattack_Y\r\n",
    "# 用的GradientBoosting\r\n",
    "shadowTrain2(clf,clf4,train_dataset1,num=10)\r\n",
    "\r\n",
    "# shadowTrain(clf,clf6,train_dataset1,num=1) #xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 攻击模型部分\n",
    "用机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------TRAIN ATTACK----------\n",
      "\n",
      "SVM:\n",
      "trianing prediction:0.800000\n",
      "testing prediction:0.800000\n",
      "漏报率: 0.19999999999999996\n",
      "误报率: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.00      0.00      0.00       100\n",
      "   malicious       0.80      1.00      0.89       400\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       500\n",
      "   macro avg       0.40      0.50      0.44       500\n",
      "weighted avg       0.64      0.80      0.71       500\n",
      "\n",
      "\n",
      "\n",
      "RandomForest:\n",
      "trianing prediction:0.878156\n",
      "testing prediction:0.680000\n",
      "漏报率: 0.31999999999999995\n",
      "误报率: 0.32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.22      0.23      0.22       100\n",
      "   malicious       0.80      0.79      0.80       400\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       500\n",
      "   macro avg       0.51      0.51      0.51       500\n",
      "weighted avg       0.69      0.68      0.68       500\n",
      "\n",
      "\n",
      "\n",
      "AdaBoost:\n",
      "trianing prediction:0.800000\n",
      "testing prediction:0.800000\n",
      "漏报率: 0.19999999999999996\n",
      "误报率: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.00      0.00      0.00       100\n",
      "   malicious       0.80      1.00      0.89       400\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       500\n",
      "   macro avg       0.40      0.50      0.44       500\n",
      "weighted avg       0.64      0.80      0.71       500\n",
      "\n",
      "\n",
      "\n",
      "GradientBoosting:\n",
      "trianing prediction:0.818517\n",
      "testing prediction:0.758000\n",
      "漏报率: 0.242\n",
      "误报率: 0.242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.26      0.11      0.15       100\n",
      "   malicious       0.81      0.92      0.86       400\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       500\n",
      "   macro avg       0.53      0.52      0.51       500\n",
      "weighted avg       0.70      0.76      0.72       500\n",
      "\n",
      "\n",
      "\n",
      "xgboost:\n",
      "trianing prediction:0.831062\n",
      "testing prediction:0.746000\n",
      "漏报率: 0.254\n",
      "误报率: 0.254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.24      0.12      0.16       100\n",
      "   malicious       0.80      0.90      0.85       400\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       500\n",
      "   macro avg       0.52      0.51      0.50       500\n",
      "weighted avg       0.69      0.75      0.71       500\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression:\n",
      "trianing prediction:0.800000\n",
      "testing prediction:0.800000\n",
      "漏报率: 0.19999999999999996\n",
      "误报率: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.00      0.00      0.00       100\n",
      "   malicious       0.80      1.00      0.89       400\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       500\n",
      "   macro avg       0.40      0.50      0.44       500\n",
      "weighted avg       0.64      0.80      0.71       500\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_attack_model(dataset):\r\n",
    "    x_train,y_train,x_test, y_test = dataset\r\n",
    "    clf1.fit(x_train,y_train)\r\n",
    "    clf2.fit(x_train,y_train)\r\n",
    "    clf3.fit(x_train,y_train)\r\n",
    "    clf4.fit(x_train,y_train)\r\n",
    "    clf6.fit(x_train,y_train)\r\n",
    "    clf7.fit(x_train,y_train)\r\n",
    "    \r\n",
    "\r\n",
    "    #模型评估\r\n",
    "    print(\"SVM:\")  \r\n",
    "    print_accuracy(clf1,x_train,y_train,x_test,y_test)\r\n",
    "    print(\"RandomForest:\")\r\n",
    "    print_accuracy(clf2,x_train,y_train,x_test,y_test)\r\n",
    "    print(\"AdaBoost:\")  \r\n",
    "    print_accuracy(clf3,x_train,y_train,x_test,y_test)\r\n",
    "    print(\"GradientBoosting:\")\r\n",
    "    print_accuracy(clf4,x_train,y_train,x_test,y_test)\r\n",
    "    # print(\"neural_network:\")  \r\n",
    "    # print_accuracy(clf5,x_train,y_train,x_test,y_test)\r\n",
    "    print(\"xgboost:\")\r\n",
    "    print_accuracy(clf6,x_train,y_train,x_test,y_test)\r\n",
    "    print(\"LogisticRegression:\")\r\n",
    "    print_accuracy(clf7,x_train,y_train,x_test,y_test)\r\n",
    "\r\n",
    "    #绘制特征的重要性\r\n",
    "    # from xgboost import plot_importance\r\n",
    "    # plot_importance(clf6)\r\n",
    "    # plt.show()\r\n",
    "\r\n",
    "\r\n",
    "# 加载数据集\r\n",
    "def load_attack_data(attack_train,attack_test):\r\n",
    "    fname = attack_train\r\n",
    "    with np.load(fname) as f:\r\n",
    "        train_x, train_y = [f['arr_%d' % i] for i in range(len(f.files))]\r\n",
    "    fname = attack_test\r\n",
    "    with np.load(fname) as f:\r\n",
    "        test_x, test_y = [f['arr_%d' % i] for i in range(len(f.files))]\r\n",
    "    # 打乱数据\r\n",
    "    index1 = [i for i in range(len(train_x))]\r\n",
    "    index2 = [i for i in range(len(test_x))] \r\n",
    "    np.random.seed(12)\r\n",
    "    np.random.shuffle(index1) # 打乱索引\r\n",
    "    np.random.seed(13)\r\n",
    "    np.random.shuffle(index2)\r\n",
    "    train_x = train_x[index1]\r\n",
    "    train_y = train_y[index1]\r\n",
    "    test_x = test_x[index2]\r\n",
    "    test_y = test_y[index2]\r\n",
    "    return train_x.astype('float32'), train_y.astype('int32'), test_x.astype('float32'), test_y.astype('int32')\r\n",
    "\r\n",
    "    \r\n",
    "print ('-' * 10 + 'TRAIN ATTACK' + '-' * 10 + '\\n')\r\n",
    "attack_train_x, attack_train_y, attack_test_x, attack_test_y=load_attack_data(\"work/Mtrainset.npz\",\"work/Mtestset.npz\")\r\n",
    "dataset = (attack_train_x, attack_train_y, attack_test_x, attack_test_y)\r\n",
    "train_attack_model(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xgbc = XGBClassifier(objective='binary:logistic')\r\n",
    "param_dist1 = {\r\n",
    "        'n_estimators':range(40,201,10)\r\n",
    "        # 'max_depth':range(4,11,2),\r\n",
    "        # 'learning_rate':np.linspace(0.01,0.2,10),\r\n",
    "        }\r\n",
    "\r\n",
    "\r\n",
    "attackclf = GridSearchCV(clf2, param_dist1, cv = 5, iid=True, scoring='roc_auc', n_jobs=-1)\r\n",
    "attackclf.fit(attack_train_x, attack_train_y) # gridSearch自带cross_validate,所以用全部数据训练即可\r\n",
    "np.save('./work/gridSearch2.npy', clf.cv_results_)\r\n",
    "attackclf.best_params_\r\n",
    "print(\"RandomForest:\")\r\n",
    "print_accuracy(attackclf,attack_train_x, attack_train_y, attack_test_x, attack_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with 24950 training data, 2 classes...\n",
      "WARNING:tensorflow:From /home/aistudio/work/attack.py:12: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "train epoch 0:\n",
      "[5.045484, 0.42316633]\n",
      "train epoch 5:\n",
      "[4.1245623, 0.5218036]\n",
      "train epoch 10:\n",
      "[3.3081493, 0.54977953]\n",
      "train epoch 15:\n",
      "[2.6748667, 0.5934269]\n",
      "train epoch 20:\n",
      "[2.1795814, 0.6282966]\n",
      "train epoch 25:\n",
      "[1.8307916, 0.6456112]\n",
      "train epoch 30:\n",
      "[1.52711, 0.6651703]\n",
      "train epoch 35:\n",
      "[1.3163058, 0.69266534]\n",
      "train epoch 40:\n",
      "[1.138999, 0.7079759]\n",
      "train epoch 45:\n",
      "[1.0392185, 0.7261723]\n",
      "train epoch 50:\n",
      "[0.9483713, 0.7402004]\n",
      "train epoch 55:\n",
      "[0.91780186, 0.7468537]\n",
      "train epoch 60:\n",
      "[0.8570015, 0.757515]\n",
      "train epoch 65:\n",
      "[0.8253831, 0.76008016]\n",
      "train epoch 70:\n",
      "[0.79527605, 0.765491]\n",
      "train epoch 75:\n",
      "[0.774076, 0.7700601]\n",
      "train epoch 80:\n",
      "[0.76283514, 0.77162325]\n",
      "train epoch 85:\n",
      "[0.74433225, 0.77182364]\n",
      "train epoch 90:\n",
      "[0.7426217, 0.7760321]\n",
      "train epoch 95:\n",
      "[0.7374828, 0.7775551]\n",
      "train epoch 100:\n",
      "[0.7272033, 0.7788377]\n",
      "train epoch 105:\n",
      "[0.7167568, 0.78032064]\n",
      "train epoch 110:\n",
      "[0.71056545, 0.78108215]\n",
      "train epoch 115:\n",
      "[0.6934354, 0.78228456]\n",
      "train epoch 120:\n",
      "[0.684713, 0.7822445]\n",
      "train epoch 125:\n",
      "[0.684031, 0.7806814]\n",
      "train epoch 130:\n",
      "[0.6680771, 0.78112227]\n",
      "train epoch 135:\n",
      "[0.660177, 0.7828056]\n",
      "train epoch 140:\n",
      "[0.64886737, 0.78176355]\n",
      "train epoch 145:\n",
      "[0.62928283, 0.7854509]\n",
      "train epoch 150:\n",
      "[0.6333551, 0.7850501]\n",
      "train epoch 155:\n",
      "[0.6172341, 0.7849699]\n",
      "train epoch 160:\n",
      "[0.58554596, 0.7866533]\n",
      "train epoch 165:\n",
      "[0.5739999, 0.7858517]\n",
      "train epoch 170:\n",
      "[0.5669296, 0.78697395]\n",
      "train epoch 175:\n",
      "[0.5681018, 0.78737473]\n",
      "train epoch 180:\n",
      "[0.56564254, 0.7875751]\n",
      "train epoch 185:\n",
      "[0.56444097, 0.7870541]\n",
      "train epoch 190:\n",
      "[0.5598131, 0.78617233]\n",
      "train epoch 195:\n",
      "[0.5537698, 0.7864529]\n",
      "train epoch 200:\n",
      "[0.5568046, 0.7858517]\n",
      "train epoch 205:\n",
      "[0.5520696, 0.7868136]\n",
      "train epoch 210:\n",
      "[0.55012107, 0.7865732]\n",
      "train epoch 215:\n",
      "[0.5495738, 0.7858517]\n",
      "train epoch 220:\n",
      "[0.5519642, 0.78601205]\n",
      "train epoch 225:\n",
      "[0.5527999, 0.78513026]\n",
      "train epoch 230:\n",
      "[0.54729444, 0.78697395]\n",
      "train epoch 235:\n",
      "[0.54446775, 0.787014]\n",
      "train epoch 240:\n",
      "[0.5452189, 0.7851703]\n",
      "train epoch 245:\n",
      "[0.54187, 0.78717434]\n",
      "train epoch 250:\n",
      "[0.540813, 0.78633267]\n",
      "train epoch 255:\n",
      "[0.54299927, 0.7861323]\n",
      "train epoch 260:\n",
      "[0.54305094, 0.7842485]\n",
      "train epoch 265:\n",
      "[0.53978866, 0.7870942]\n",
      "train epoch 270:\n",
      "[0.5370185, 0.78797597]\n",
      "train epoch 275:\n",
      "[0.5371836, 0.7870541]\n",
      "train epoch 280:\n",
      "[0.5353085, 0.78717434]\n",
      "train epoch 285:\n",
      "[0.5352743, 0.79034066]\n",
      "train epoch 290:\n",
      "[0.53576374, 0.79054105]\n",
      "train epoch 295:\n",
      "[0.5355566, 0.7888577]\n",
      "train epoch 300:\n",
      "[0.5312653, 0.78893787]\n",
      "train epoch 305:\n",
      "[0.53417814, 0.78981966]\n",
      "train epoch 310:\n",
      "[0.53249204, 0.79098195]\n",
      "train epoch 315:\n",
      "[0.53155947, 0.7927054]\n",
      "train epoch 320:\n",
      "[0.5303942, 0.7919038]\n",
      "train epoch 325:\n",
      "[0.5293792, 0.7909018]\n",
      "train epoch 330:\n",
      "[0.5296621, 0.7919439]\n",
      "train epoch 335:\n",
      "[0.52944237, 0.7918236]\n",
      "train epoch 340:\n",
      "[0.52803814, 0.79154307]\n",
      "train epoch 345:\n",
      "[0.5299857, 0.79166335]\n",
      "train epoch 350:\n",
      "[0.5275775, 0.79218435]\n",
      "train epoch 355:\n",
      "[0.5266725, 0.7949499]\n",
      "train epoch 360:\n",
      "[0.5252688, 0.7951904]\n",
      "train epoch 365:\n",
      "[0.52637345, 0.79446894]\n",
      "train epoch 370:\n",
      "[0.52367353, 0.7957114]\n",
      "train epoch 375:\n",
      "[0.52327603, 0.7957114]\n",
      "train epoch 380:\n",
      "[0.5249375, 0.79543084]\n",
      "train epoch 385:\n",
      "[0.5230864, 0.79482967]\n",
      "train epoch 390:\n",
      "[0.5240613, 0.7951102]\n",
      "train epoch 395:\n",
      "[0.5214532, 0.7977154]\n",
      "train epoch 400:\n",
      "[0.5242792, 0.79663324]\n",
      "train epoch 405:\n",
      "[0.5209893, 0.7970341]\n",
      "train epoch 410:\n",
      "[0.523669, 0.79623246]\n",
      "train epoch 415:\n",
      "[0.5230879, 0.79663324]\n",
      "train epoch 420:\n",
      "[0.5219016, 0.7963527]\n",
      "train epoch 425:\n",
      "[0.52328455, 0.7967936]\n",
      "train epoch 430:\n",
      "[0.52202106, 0.7963527]\n",
      "train epoch 435:\n",
      "[0.5209186, 0.7970341]\n",
      "train epoch 440:\n",
      "[0.52198845, 0.796994]\n",
      "train epoch 445:\n",
      "[0.5204978, 0.79655313]\n",
      "train epoch 450:\n",
      "[0.5213644, 0.79655313]\n",
      "train epoch 455:\n",
      "[0.51950276, 0.7968337]\n",
      "train epoch 460:\n",
      "[0.52100295, 0.7969539]\n",
      "train epoch 465:\n",
      "[0.5188889, 0.7974749]\n",
      "train epoch 470:\n",
      "[0.5194351, 0.79611224]\n",
      "train epoch 475:\n",
      "[0.5207792, 0.79643285]\n",
      "train epoch 480:\n",
      "[0.51900095, 0.79667336]\n",
      "train epoch 485:\n",
      "[0.5184241, 0.7977956]\n",
      "train epoch 490:\n",
      "[0.52103287, 0.797996]\n",
      "train epoch 495:\n",
      "[0.5195827, 0.7985571]\n",
      "train epoch 500:\n",
      "[0.5190653, 0.7985571]\n",
      "train epoch 505:\n",
      "[0.51952523, 0.7985972]\n",
      "train epoch 510:\n",
      "[0.51885563, 0.7982365]\n",
      "train epoch 515:\n",
      "[0.5185463, 0.7982365]\n",
      "train epoch 520:\n",
      "[0.5189995, 0.79827654]\n",
      "train epoch 525:\n",
      "[0.5175504, 0.7985972]\n",
      "train epoch 530:\n",
      "[0.5189071, 0.79871744]\n",
      "train epoch 535:\n",
      "[0.51590604, 0.79895794]\n",
      "train epoch 540:\n",
      "[0.5169738, 0.79847693]\n",
      "train epoch 545:\n",
      "[0.5185726, 0.7993988]\n",
      "train epoch 550:\n",
      "[0.51847625, 0.7988778]\n",
      "train epoch 555:\n",
      "[0.5187341, 0.7996393]\n",
      "train epoch 560:\n",
      "[0.5175686, 0.79947895]\n",
      "train epoch 565:\n",
      "[0.5191262, 0.79895794]\n",
      "train epoch 570:\n",
      "[0.517009, 0.7991984]\n",
      "train epoch 575:\n",
      "[0.5177656, 0.798998]\n",
      "train epoch 580:\n",
      "[0.51586837, 0.7992385]\n",
      "train epoch 585:\n",
      "[0.5179412, 0.7987976]\n",
      "train epoch 590:\n",
      "[0.51813936, 0.7986373]\n",
      "train epoch 595:\n",
      "[0.51880157, 0.7991182]\n",
      "train epoch 600:\n",
      "[0.5162978, 0.79903805]\n",
      "train epoch 605:\n",
      "[0.51542246, 0.79967934]\n",
      "train epoch 610:\n",
      "[0.51755244, 0.7995591]\n",
      "train epoch 615:\n",
      "[0.5171991, 0.7991984]\n",
      "train epoch 620:\n",
      "[0.51717323, 0.79915833]\n",
      "train epoch 625:\n",
      "[0.51758784, 0.79951906]\n",
      "train epoch 630:\n",
      "[0.51764417, 0.79903805]\n",
      "train epoch 635:\n",
      "[0.51648825, 0.7993587]\n",
      "train epoch 640:\n",
      "[0.518055, 0.7987575]\n",
      "train epoch 645:\n",
      "[0.5190699, 0.7992385]\n",
      "train epoch 650:\n",
      "[0.51778615, 0.79903805]\n",
      "train epoch 655:\n",
      "[0.5181623, 0.7991182]\n",
      "train epoch 660:\n",
      "[0.5171909, 0.79903805]\n",
      "train epoch 665:\n",
      "[0.51853484, 0.7993587]\n",
      "train epoch 670:\n",
      "[0.51924247, 0.7993587]\n",
      "train epoch 675:\n",
      "[0.51625496, 0.7996393]\n",
      "train epoch 680:\n",
      "[0.5163902, 0.7995591]\n",
      "train epoch 685:\n",
      "[0.5171287, 0.79891783]\n",
      "train epoch 690:\n",
      "[0.51982737, 0.798998]\n",
      "train epoch 695:\n",
      "[0.517558, 0.7993186]\n",
      "train epoch 700:\n",
      "[0.51620126, 0.7992385]\n",
      "train epoch 705:\n",
      "[0.51736736, 0.79871744]\n",
      "train epoch 710:\n",
      "[0.51692563, 0.79915833]\n",
      "train epoch 715:\n",
      "[0.517527, 0.7995591]\n",
      "train epoch 720:\n",
      "[0.5187116, 0.7991182]\n",
      "train epoch 725:\n",
      "[0.51639074, 0.798998]\n",
      "train epoch 730:\n",
      "[0.5171782, 0.79907817]\n",
      "train epoch 735:\n",
      "[0.51806384, 0.7991182]\n",
      "train epoch 740:\n",
      "[0.51799405, 0.79895794]\n",
      "train epoch 745:\n",
      "[0.5172498, 0.79947895]\n",
      "train epoch 750:\n",
      "[0.5176566, 0.79927856]\n",
      "train epoch 755:\n",
      "[0.5177133, 0.79851705]\n",
      "train epoch 760:\n",
      "[0.51771444, 0.7993587]\n",
      "train epoch 765:\n",
      "[0.5161099, 0.7991182]\n",
      "train epoch 770:\n",
      "[0.51811475, 0.7991984]\n",
      "train epoch 775:\n",
      "[0.5183293, 0.79847693]\n",
      "train epoch 780:\n",
      "[0.5164507, 0.7991984]\n",
      "train epoch 785:\n",
      "[0.51844424, 0.79871744]\n",
      "train epoch 790:\n",
      "[0.5157041, 0.798998]\n",
      "train epoch 795:\n",
      "[0.51675546, 0.7995591]\n",
      "train epoch 800:\n",
      "[0.516131, 0.79895794]\n",
      "train epoch 805:\n",
      "[0.5149257, 0.7986773]\n",
      "train epoch 810:\n",
      "[0.5175892, 0.7992385]\n",
      "train epoch 815:\n",
      "[0.51564866, 0.7991182]\n",
      "train epoch 820:\n",
      "[0.5156107, 0.7993186]\n",
      "train epoch 825:\n",
      "[0.514356, 0.7993988]\n",
      "train epoch 830:\n",
      "[0.51531374, 0.79891783]\n",
      "train epoch 835:\n",
      "[0.5166348, 0.7988778]\n",
      "train epoch 840:\n",
      "[0.5155778, 0.7993186]\n",
      "train epoch 845:\n",
      "[0.515882, 0.79951906]\n",
      "train epoch 850:\n",
      "[0.51619303, 0.7991984]\n",
      "train epoch 855:\n",
      "[0.5154705, 0.7993186]\n",
      "train epoch 860:\n",
      "[0.5158891, 0.80004007]\n",
      "train epoch 865:\n",
      "[0.5156897, 0.8]\n",
      "train epoch 870:\n",
      "[0.5172113, 0.8]\n",
      "train epoch 875:\n",
      "[0.5173053, 0.8]\n",
      "train epoch 880:\n",
      "[0.51577973, 0.8]\n",
      "train epoch 885:\n",
      "[0.516413, 0.8]\n",
      "train epoch 890:\n",
      "[0.5160014, 0.8]\n",
      "train epoch 895:\n",
      "[0.5173062, 0.8]\n",
      "train epoch 900:\n",
      "[0.51548153, 0.8]\n",
      "train epoch 905:\n",
      "[0.5164604, 0.80004007]\n",
      "train epoch 910:\n",
      "[0.5166247, 0.8]\n",
      "train epoch 915:\n",
      "[0.517318, 0.80004007]\n",
      "train epoch 920:\n",
      "[0.51633424, 0.8]\n",
      "train epoch 925:\n",
      "[0.51665, 0.8000802]\n",
      "train epoch 930:\n",
      "[0.51633555, 0.80004007]\n",
      "train epoch 935:\n",
      "[0.517044, 0.8]\n",
      "train epoch 940:\n",
      "[0.51588964, 0.80004007]\n",
      "train epoch 945:\n",
      "[0.51455003, 0.8]\n",
      "train epoch 950:\n",
      "[0.5142548, 0.80004007]\n",
      "train epoch 955:\n",
      "[0.5169434, 0.80004007]\n",
      "train epoch 960:\n",
      "[0.5152512, 0.8]\n",
      "train epoch 965:\n",
      "[0.5161051, 0.8]\n",
      "train epoch 970:\n",
      "[0.51615244, 0.8]\n",
      "train epoch 975:\n",
      "[0.517799, 0.8]\n",
      "train epoch 980:\n",
      "[0.5168085, 0.80004007]\n",
      "train epoch 985:\n",
      "[0.5189479, 0.8]\n",
      "train epoch 990:\n",
      "[0.5164792, 0.80004007]\n",
      "train epoch 995:\n",
      "[0.51568604, 0.80004007]\n",
      "Testing...\n",
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 再试试用全连接网络\r\n",
    "from  work.attack import train as attack_model\r\n",
    "attack_train_x1,attack_train_y1,attack_test_x1,attack_test_y1=load_attack_data(\"work/trainset.npz\",\"work/testset.npz\")\r\n",
    "dataset = (attack_train_x1, attack_train_y1, attack_test_x1, attack_test_y1)\r\n",
    "acc= attack_model(dataset,epochs=500, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "提示：请在一行中输入包括Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses在内的10个指标, 仅用英文逗号隔开.\n",
      "\n",
      "请输入: 您输入的指标对应为:\n",
      "+-----------------------------+--------+\n",
      "| Attribute                   | Domain |\n",
      "+-----------------------------+--------+\n",
      "| Clump Thickness             | 0      |\n",
      "+-----------------------------+--------+\n",
      "| Uniformity of Cell Size     | 1      |\n",
      "+-----------------------------+--------+\n",
      "| Uniformity of Cell Shape    | 2      |\n",
      "+-----------------------------+--------+\n",
      "| Marginal Adhesion           | 3      |\n",
      "+-----------------------------+--------+\n",
      "| Single Epithelial Cell Size | 4      |\n",
      "+-----------------------------+--------+\n",
      "| Bare Nuclei                 | 5      |\n",
      "+-----------------------------+--------+\n",
      "| Bland Chromatin             | 6      |\n",
      "+-----------------------------+--------+\n",
      "| Normal Nucleoli             | 7      |\n",
      "+-----------------------------+--------+\n",
      "| Mitoses                     | 8      |\n",
      "+-----------------------------+--------+\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8']\nexpected Uniformity of Cell Size, Normal Nucleoli, Marginal Adhesion, Clump Thickness, Uniformity of Cell Shape, Bland Chromatin, Bare Nuclei, Mitoses, Single Epithelial Cell Size in input data\ntraining data did not have the following fields: f3, f4, f6, f0, f1, f8, f2, f5, f7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-969908e397d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTexttable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Benign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Malignant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m    935\u001b[0m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[1;32m    936\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                                                  validate_features=validate_features)\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1854\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     def get_split_value_histogram(self, feature, fmap='', bins=None,\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8']\nexpected Uniformity of Cell Size, Normal Nucleoli, Marginal Adhesion, Clump Thickness, Uniformity of Cell Shape, Bland Chromatin, Bare Nuclei, Mitoses, Single Epithelial Cell Size in input data\ntraining data did not have the following fields: f3, f4, f6, f0, f1, f8, f2, f5, f7"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\r\n",
    "# \r\n",
    "\r\n",
    "from texttable import Texttable\r\n",
    "clf=joblib.load(\"./work/xgbc_model.pkl\")\r\n",
    "\r\n",
    "while(1):\r\n",
    "    print(\"=\"*100)\r\n",
    "    print(\"提示：请在一行中输入包括Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses在内的10个指标, 仅用英文逗号隔开.\\n\")\r\n",
    "    test_data = input(\"请输入: \")\r\n",
    "\r\n",
    "    if test_data == \"quit\":\r\n",
    "        break\r\n",
    "    \r\n",
    "    test_data = test_data.split(',')\r\n",
    "    test_data = [int(e) for e in test_data]\r\n",
    "\r\n",
    "    print(\"您输入的指标对应为:\")\r\n",
    "    t = Texttable()\r\n",
    "    col_names = [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\"]\r\n",
    "    _ = t.add_row([\"Attribute\", \"Domain\"])\r\n",
    "    for index in range(len(col_names)):\r\n",
    "        _ = t.add_row([col_names[index], test_data[index]])\r\n",
    "    print(t.draw())\r\n",
    "\r\n",
    "    test_df= pd.DataFrame([test_data], columns=[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\",\"Normal Nucleoli\", \"Mitoses\"])\r\n",
    "    test_df = ss.transform(test_df)\r\n",
    "\r\n",
    "    x_proba = clf.predict_proba(test_df)\r\n",
    "    t2 = Texttable()\r\n",
    "    _ = t2.add_rows([[\"Benign\", \"Malignant\"], [x_proba[0][0], x_proba[0][1]]])\r\n",
    "    print(\"\\n您的检测结果为: \")\r\n",
    "    print(t2.draw())\r\n",
    "# 这里我是仿造前面部分写的，但是输入10个指标或者8个指标都报错，不知道什么原因\r\n",
    "    label=input(\"请输入对应真实标签: \")\r\n",
    "    x_proba.append(label)\r\n",
    "    x1_proba=attackclf.predic_proba(x_proba)\r\n",
    "    t3 = Texttable()\r\n",
    "    _ = t3.add_rows([[\"member\", \"nomember\"], [x1_proba[0][0], x1_proba[0][1]]])\r\n",
    "    print(\"\\n身份结果为: \")\r\n",
    "    print(t3.draw())\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
